{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Objectives\n",
    "1. The primary objective of this program is to determine whether the mixed audio can be accurately recreated using the mixing coefficients provided in the song collection of the BandHub dataset. If this is the case, the mixed audio need not be stored and can be recreated as and when required, thereby saving us significant space on the file server.\n",
    "\n",
    "2. In a handful of cases, the same mix has two mixing coefficients. One in the settings dictionary, and the other in the audioChannels list, which is within the settings dictionary. Most times these are just approximation errors. However, in some cases the differences are significant. Therefore, the secondary objective is to clarify which of these multiple mixing coefficients is more accurate.\n",
    "\n",
    "## Code Organization\n",
    "1. Imports\n",
    "2. Connect to mongoDB client\n",
    "3. Get all collections: Songs, Videos, Tracks, and Posts\n",
    "4. Function definitions\n",
    " * All functions have been elaborated on in the markdown cells above them.\n",
    "5. Main script\n",
    " * This script calls all the functions as and when required to accomplish both objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(30000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 30 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold='nan')\n",
    "from numpy.linalg import inv\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from bson.objectid import ObjectId\n",
    "import json\n",
    "from pprint import pprint\n",
    "from pydub import AudioSegment\n",
    "import urllib\n",
    "import fnmatch\n",
    "import os\n",
    "import requests\n",
    "import ffmpy\n",
    "import subprocess\n",
    "import librosa\n",
    "import librosa.display\n",
    "import time\n",
    "import IPython.display as ipd\n",
    "import pescador\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "DATA_ROOT = '/scratch/rrs432/openmic/openmic-2018'\n",
    "\n",
    "if not os.path.exists(DATA_ROOT):\n",
    "    raise ValueError('Did you forget to set `DATA_ROOT`?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Connect to mongoDB client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('localhost',27017)\n",
    "#client = pymongo.MongoClient('localhost',32768)\n",
    "db = client.get_database('b=bandhub')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get all collections: Songs, Videos, Tracks, and Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[425706, 198169, 915582, 494867]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songCol = db.get_collection('songsStream')\n",
    "vidCol = db.get_collection('mergedVideos')\n",
    "trackCol = db.get_collection('tracksStream')\n",
    "postCol = db.get_collection('posts')\n",
    "[songCol.count(), vidCol.count(), trackCol.count(), postCol.count()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this dataset, we are only concerned with tracks that have \"public\" access. This is denoted by the variable \"songAccess\", which and is set to 1 for public tracks.<br>\n",
    "Conversely, many mixes have a lot of unused public tracks. The tracks that actually make the final mix are titled \"published\" tracks. Information about the published tracks in a mix are available only in the post collection, and so this function queries the post collection to return the relevant documents.<br>\n",
    "To summarize, this generator returns the documents in the post collection that have a user-specified number (num_track) of published tracks. For our evaluation purposes, we have restricted num_track to be greater than 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_published_tracks(num_track):\n",
    "    \"\"\" This generator function returns all post documents with \"num_track\" published tracks, where num_track > 2.\n",
    "    ----------\n",
    "    Arguments:\n",
    "        num_track: Number of public, published tracks per mix\n",
    "\n",
    "    Returns:\n",
    "        docs: All documents from the post collection that satisfy the aforementioned condition\n",
    "    \"\"\"\n",
    "    \n",
    "    pub_tracks = postCol.find({'songAccess':{'$exists': True}}, {'participantsInfo':1 ,'songAccess':1, 'objectId':1})\n",
    "    for docs in pub_tracks:\n",
    "        if(docs['songAccess'] == 1):\n",
    "            if(len(docs['participantsInfo']['publishedTracks']) == num_track):\n",
    "                yield docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function aids in achieving the objective of identifying whether bandhub's mixes can be accurately recreated using the given mixing coefficients. First, it sifts through all the documents returned from the get_published_tracks function. Then, it checks and keeps a track of any mixes that have missing published track information. If all the information is present, the post ID and the song ID of the mix are appended to a list and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_audio_ids(num_track):\n",
    "    \"\"\" This function computes a list of the post IDs & master song IDs of those mixes generated by the\n",
    "    get_published_tracks function that contain multiple mixing coefficients associated with the same mix.\n",
    "    ----------\n",
    "    Arguments:\n",
    "        num_track: Number of public, published tracks per mix\n",
    "\n",
    "    Yields:\n",
    "        pub_songs: A list of tuples containing relevant post & track IDs\n",
    "    \"\"\"\n",
    "    \n",
    "    pub_docs = [x for x in get_published_tracks(num_track)]\n",
    "    count_mixes = len(pub_docs)\n",
    "    print(\"Number of mixes with \" + str(num_track) + \" published tracks: \" + str(count_mixes))\n",
    "    missing_song_id = []\n",
    "    missing_pubtrack = []\n",
    "    index_required = []\n",
    "    pub_songs = []\n",
    "    for j in range(count_mixes):\n",
    "        pub_tracks = []\n",
    "        post_id = pub_docs[j]['_id']\n",
    "        song_id = pub_docs[j]['objectId']\n",
    "        \n",
    "        song_look_up = songCol.find({'masterSongId' : song_id})\n",
    "        \n",
    "        for i in range(num_track):\n",
    "            pub_tracks.append(pub_docs[j]['participantsInfo']['publishedTracks'][i]['_id'])\n",
    "                                \n",
    "        if song_look_up is -1:\n",
    "            missing_song_id.append(song_id)\n",
    "            pprint('Error: SongID not found in Song Collection!')\n",
    "        else:\n",
    "            for song_docs in song_look_up:\n",
    "                count_pub_track = 0\n",
    "                for i in range(len(pub_tracks)):\n",
    "                    if(str(pub_tracks[i]) not in song_docs['settings']):\n",
    "                        missing_pubtrack.append(pub_tracks[i])   \n",
    "                        break\n",
    "                    else:\n",
    "                        count_pub_track+=1\n",
    "                if count_pub_track == num_track:\n",
    "                    pub_songs.append((post_id,song_id))\n",
    "\n",
    "    return pub_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file URLs in the dataset are of the following format:<br>\n",
    "http://bandhubwebmedia1.blob.core.windows.net/files/520c0d243262aac9f04d4351-520c0d8753294aac9f04d4353.ogg<br>\n",
    "We can see that the filename is present in this string after the last separator (/). This function strips the file name from the given file URL and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(file):\n",
    "    \"\"\" This function strips the filename along with the file extension from the file URL.\n",
    "    ----------\n",
    "    Arguments:\n",
    "        file: file URL\n",
    "\n",
    "    Returns:\n",
    "        filepath: file path\n",
    "    \"\"\"\n",
    "    \n",
    "    filePath = file.split('/')\n",
    "    return filePath[- 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mixing coefficients necessary to recreate a mix are the following:\n",
    "* pub_t_vol: Track volume in the settings dictionary\n",
    "* ogg_paths: File paths of the tracks on the server.\n",
    "* start_times: Track start times<br>\n",
    "Given a song ID and post ID unique to a mix, this function queries the corresponding collections and extracts the above information for every published track in the mix.<br>\n",
    "The track volumes are all rounded to 2 decimal places. The file URL is fetched from the effectsAudioURL field, which provides processed tracks (i.e., tracks with effects and processing). If not found, it is fetched from the fileURL field, which provides the raw track file.<br>\n",
    "The start times for each track are queried from the track collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mixing_coeff(num_track, song_id, post_id):\n",
    "    \"\"\" This function gathers all mixing coefficients necessary to remix a song using its constituent\n",
    "    published tracks.\n",
    "    ----------\n",
    "    Arguments:\n",
    "        num_track: Number of public, published tracks per mix\n",
    "        song_id: Song ID\n",
    "        post_id: Post ID\n",
    "\n",
    "    Returns:\n",
    "        pub_t_vol: Published track volumes from the settings field in the song collection\n",
    "        ogg_paths: Processed (if available) or unprocessed file URLs of each published track\n",
    "    \"\"\"\n",
    "    \n",
    "    pub_t_vol = []\n",
    "    ogg_paths = []\n",
    "    track_filenames = []\n",
    "    pub_tracks = []\n",
    "\n",
    "    song_doc = songCol.find({'masterSongId' : song_id})\n",
    "    post_doc = postCol.find({'_id' : post_id})\n",
    "    \n",
    "    for docs in post_doc:\n",
    "        for i in range(num_track):\n",
    "            pub_tracks.append(docs['participantsInfo']['publishedTracks'][i]['_id'])\n",
    "    \n",
    "    for docs in song_doc:\n",
    "        for i in range(len(pub_tracks)):\n",
    "            if(str(pub_tracks[i]) in docs['settings']):\n",
    "                pub_t_vol.append(round(docs['settings'][str(pub_tracks[i])]['volume'],2))            \n",
    "                gettrack = trackCol.find({'_id' : pub_tracks[i]})\n",
    "                for tdocs in gettrack:\n",
    "                    if('effectsAudioUrl' in docs['settings'][str(pub_tracks[i])]):             \n",
    "                        ogg_paths.append(docs['settings'][str(pub_tracks[i])]['effectsAudioUrl'])\n",
    "                        #print('Processed audio URL')\n",
    "                    else:\n",
    "                        ogg_paths.append(tdocs['audioChannels'][0]['fileUrl'])\n",
    "                        #print('Unprocessed audio URL')\n",
    "            else:\n",
    "                print('Missing published track!')\n",
    "        \n",
    "    return pub_t_vol, ogg_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function changes the default working directory to a user-specified folder in the user's scratch directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_path():\n",
    "    \"\"\" This function sets the default download path to a subfolder within the user's scratch folder.\n",
    "    \"\"\"\n",
    "\n",
    "    path = '/scratch/rrs432/openmic/openmic-2018/mixedAudio'\n",
    "    os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the file's path on the server, this function downloads the file (as is) onto the local working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_track(ogg_path):\n",
    "    \"\"\" This function downloads the track using its filepath to the default download folder.\n",
    "    ----------\n",
    "    Arguments:\n",
    "        ogg_path: file path\n",
    "    \"\"\"    \n",
    "    set_path()\n",
    "\n",
    "    r = requests.get(ogg_path)\n",
    "    filename = get_filename(ogg_path)\n",
    "    with open(filename, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=128):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function clears all previously downloaded songs/tracks in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_downloads():\n",
    "    \"\"\" This function clears all previously downloaded songs/tracks in default download folder.\n",
    "    \"\"\"\n",
    "    ! rm -rf *.ogg *.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes the estimated mix by multiplying the track volumes with their corresponding track data and summing the results.<br>\n",
    "**Note:** In this notebook, the term *estimated mix* refers to the mix that we manually compute using the given mixing coefficients. Whereas, *bandhub_mix* refers to the mix provided by Bandhub which is coupled with the mixed video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_estimated_mix(vol, tracks):\n",
    "    \"\"\" This function computes the estimated mix using relevant mixing coefficients.\n",
    "    ----------\n",
    "    Arguments:\n",
    "        vol: Track volume\n",
    "        tracks: Track files\n",
    "\n",
    "    Returns:\n",
    "        mix_flac: Estimated normalized mix file\n",
    "    \"\"\" \n",
    "    sr = 44100\n",
    "    num_track = len(tracks)\n",
    "    mix_flac = 0\n",
    "    for i in range(num_track):\n",
    "        if tracks[i].ndim is not 2:\n",
    "            tracks[i] = np.stack((tracks[i], tracks[i]), axis = 1)\n",
    "        mix_flac += (vol[i]*np.array(tracks[i]))\n",
    "    \n",
    "    abs_mix_flac = mix_flac / np.max(np.abs(mix_flac))\n",
    "    \n",
    "    if abs_mix_flac.shape[0] - 10*sr < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return mix_flac / np.max(np.abs(mix_flac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_features(arr):\n",
    "    \"\"\" This function reduce the dimensionality of the feature space using statistical methods\n",
    "    ----------\n",
    "    Arguments:\n",
    "        arr: Full feature array\n",
    "\n",
    "    Returns:\n",
    "        -: Reduced dimension feature array\n",
    "    \"\"\" \n",
    "    mean = np.mean(arr, axis=1)\n",
    "    std = np.std(arr, axis=1)\n",
    "    mini = np.min(arr, axis=1)\n",
    "    maxi = np.max(arr, axis=1)\n",
    "    return np.concatenate((mean,std,mini,maxi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_track_inst(npy_file_path, track_filename):\n",
    "    \"\"\" This function performs instrument prediction on a single track\n",
    "    ----------\n",
    "    Arguments:\n",
    "        npy_file_path: Path of the .npy files\n",
    "        track_filename: Filename of the track\n",
    "\n",
    "    Returns:\n",
    "        Y : dict of predicted true & false values for entire instrument pool\n",
    "    \"\"\" \n",
    "    Y = []\n",
    "    \n",
    "    filename, file_extension = os.path.splitext(track_filename) \n",
    "    mfcc_file_path = npy_file_path+filename + '.npy'\n",
    "    dmfcc_file_path = npy_file_path+filename+'_d.npy'\n",
    "    ddmfcc_file_path = npy_file_path+filename+'_dd.npy'\n",
    "    \n",
    "    if os.path.isfile(dmfcc_file_path) is True and os.path.isfile(ddmfcc_file_path) is True:\n",
    "        mfcc_file = np.load(mfcc_file_path)\n",
    "        dmfcc_file = np.load(dmfcc_file_path)\n",
    "        ddmfcc_file  = np.load(ddmfcc_file_path)\n",
    "\n",
    "        #Compute the index offset\n",
    "        if len(mfcc_file[1]) < 862:\n",
    "            return -1\n",
    "        else:\n",
    "            idx = np.random.randint(len(mfcc_file[1]) - 862)\n",
    "            mfcc_ten_second = mfcc_file[:,idx:idx + 862]\n",
    "            dmfcc_ten_second = dmfcc_file[:,idx:idx + 862]\n",
    "            ddmfcc_ten_second = ddmfcc_file[:,idx:idx + 862]\n",
    "\n",
    "        X = np.concatenate((aggregate_features(mfcc_ten_second),aggregate_features(dmfcc_ten_second),aggregate_features(ddmfcc_ten_second)))\n",
    "        X = X[np.newaxis,:]\n",
    "        \n",
    "    loaded_model = joblib.load('/scratch/rrs432/openmic/openmic-2018/finalized_model.sav')\n",
    "    for key in loaded_model:\n",
    "        Y.append(loaded_model[key].predict_proba(X))\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_track_labels(num_track, mix, instrument):\n",
    "    \"\"\" This function consolidates the labels on all the constituent tracks of the mix\n",
    "    ----------\n",
    "    Arguments:\n",
    "        num_track: Number of public, published tracks per mix\n",
    "        mix: Mix file\n",
    "        instrument: instrument key\n",
    "\n",
    "    Returns:\n",
    "        Y : dict of predicted true & false values for entire instrument pool\n",
    "    \"\"\"\n",
    "    if np.shape(mix) == (num_track, 20, 1, 2):\n",
    "        max_prob = max([mix[track][instrument][0][1] for track in range(len(mix))])\n",
    "        if max_prob > 0.5:\n",
    "            instrument_present = True\n",
    "        else:\n",
    "            instrument_present = False\n",
    "        return [instrument_present]\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mix_training_data(num_track, proc_id, instrument):\n",
    "    count = 0\n",
    "    error_flag = 0\n",
    "    track_filenames = []\n",
    "    flac_tracks = []\n",
    "\n",
    "    post_id = proc_id[0]\n",
    "    song_id = proc_id[1]\n",
    "\n",
    "    #Get mixing coefficients and filepaths\n",
    "    volume_settings, ogg_paths = get_mixing_coeff(num_track, song_id, post_id)\n",
    "\n",
    "    if len(ogg_paths) is not num_track:\n",
    "        error_flag = -1\n",
    "    else:\n",
    "        for j in range(num_track):\n",
    "            #Get filenames\n",
    "            track_filenames.append(get_filename(ogg_paths[j])) \n",
    "\n",
    "    if len(track_filenames) is not num_pub_track:\n",
    "        error_flag = -2\n",
    "    else:\n",
    "        Y_track = []\n",
    "        for j in range(len(track_filenames)):\n",
    "            filename, file_extension = os.path.splitext(track_filenames[j])\n",
    "            npy_filename = filename + '.npy'\n",
    "            flac_filename = filename + '.flac'\n",
    "\n",
    "            #Check if track is unprocessed, then check if the downloaded flac file is available\n",
    "            if os.path.isfile('/scratch/rrs432/openmic/openmic-2018/unprocessedAudio/'+npy_filename):\n",
    "                npy_file_path = '/scratch/rrs432/openmic/openmic-2018/unprocessedAudio/'\n",
    "                Y_track.append(predict_track_inst(npy_file_path, track_filenames[j]))\n",
    "                if os.path.isfile('/scratch/work/marl/bandhub/unprocessedAudio/'+flac_filename):\n",
    "                    flac_track, mixsr = sf.read('/scratch/work/marl/bandhub/unprocessedAudio/'+flac_filename)\n",
    "                    flac_tracks.append(flac_track)\n",
    "\n",
    "            #Else check if track is processed, then check if the downloaded flac file is available\n",
    "            elif os.path.isfile('/scratch/rrs432/openmic/openmic-2018/processedAudio/'+npy_filename):\n",
    "                npy_file_path = '/scratch/rrs432/openmic/openmic-2018/processedAudio/'\n",
    "                Y_track.append(predict_track_inst(npy_file_path, track_filenames[j]))\n",
    "                if os.path.isfile('/scratch/work/marl/bandhub/processedAudio/'+flac_filename):\n",
    "                    flac_track, mixsr = sf.read('/scratch/work/marl/bandhub/processedAudio/'+flac_filename)\n",
    "                    flac_tracks.append(flac_track)\n",
    "            else:\n",
    "                print('Flac file not found!') \n",
    "\n",
    "    if any(Y_track) is -1 or error_flag is not 0:\n",
    "        X = 0\n",
    "        Y = 0\n",
    "    else:\n",
    "        #Feature extraction on estimated bandhub mix\n",
    "        if len(flac_tracks) is num_track:\n",
    "            mix_flac = compute_estimated_mix(volume_settings, flac_tracks)\n",
    "            if mix_flac is not -1:\n",
    "                #Pick random 10-second snippet from the mix\n",
    "                idx = np.random.randint(mix_flac.shape[0] - 10*mixsr)\n",
    "                mix_flac = mix_flac[idx:idx + 10*mixsr,:]\n",
    "\n",
    "                if mix_flac.ndim is 2:\n",
    "                    mix_flac = mix_flac.sum(axis=1)/2\n",
    "\n",
    "                mfcc_ten_second = librosa.feature.mfcc(y=mix_flac, sr=mixsr)\n",
    "                dmfcc_ten_second = librosa.feature.delta(mfcc_ten_second)\n",
    "                ddmfcc_ten_second = librosa.feature.delta(mfcc_ten_second,order=2)\n",
    "\n",
    "                X = np.concatenate((aggregate_features(mfcc_ten_second),aggregate_features(dmfcc_ten_second),aggregate_features(ddmfcc_ten_second)))\n",
    "                X = X[np.newaxis,:]\n",
    "            else:\n",
    "                error_flag = -3\n",
    "\n",
    "        #Determine Y values for the mix by combining instrument recognition probababilities of constituent tracks\n",
    "        if error_flag is 0:\n",
    "            Y = consolidate_track_labels(num_pub_track, Y_track, instrument)\n",
    "        else:\n",
    "            X = 0\n",
    "            Y = 0\n",
    "    print('Success')\n",
    "    yield dict(X=X,Y=Y)\n",
    "# pprint(get_mix_training_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_ROOT, 'class-map.json'), 'r') as f:\n",
    "    class_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# This dictionary will include the classifiers for each model\n",
    "models = dict()\n",
    "proc_id_filepath = '/scratch/rrs432/openmic/openmic-2018/proc_id/'\n",
    "# We'll iterate over all instrument classes, and fit a model for each one\n",
    "instrumentIndex = 0\n",
    "for instrument in class_map:\n",
    "    \n",
    "    inst_num = class_map[instrument]\n",
    "        \n",
    "    # Initialize a new classifier\n",
    "    clf = RandomForestClassifier(max_depth=5, max_features= 10,n_estimators=50)\n",
    "    \n",
    "    # Iterate through all files and train\n",
    "    for num_pub_track in range(2,20):\n",
    "        npy_files = []\n",
    "        for root, dirs, files in os.walk(proc_id_filepath+str(num_pub_track)):\n",
    "            for file in files:\n",
    "                if file.endswith(\".npy\"):\n",
    "                    npy_files.append(np.load(proc_id_filepath+str(num_pub_track)+'/'+file))\n",
    "\n",
    "        streams = [pescador.Streamer(get_mix_training_data, num_pub_track, proc_id, instrumentIndex) for proc_id in npy_files]\n",
    "\n",
    "    # Keep 16 streams alive at once\n",
    "    # Draw on average 8 patches from each stream before deactivating\n",
    "    mux_stream = pescador.mux.Mux(streams, k=16, rate=8)\n",
    "\n",
    "    for batch in mux_stream(max_iter=1000):\n",
    "        if batch['X'] is not 0 and batch['Y'] is not 0:\n",
    "            clf.fit(batch['X'],batch['Y'])\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # Store the classifier in our dictionary\n",
    "    models[instrument] = clf\n",
    "    instrumentIndex += 1\n",
    "    \n",
    "# Save model to disk\n",
    "filename = 'bandhub_model.sav'\n",
    "joblib.dump(models, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
